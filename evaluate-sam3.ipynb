{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b105949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import PIL.Image as Image\n",
    "\n",
    "data = json.loads(open('USIS10K\\\\multi_class_annotations\\\\multi_class_test_annotations.json').read())\n",
    "\n",
    "id_to_img = {}\n",
    "\n",
    "for img in data['images']:\n",
    "    id_to_img[img['id']] = img['file_name']\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from pycocotools import mask as mask_utils\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "PROMPT_TO_CATEGORY = {\n",
    "    \"wrecks/ruins\": 1,\n",
    "    \"fish\": 2,\n",
    "    \"reefs\": 3,\n",
    "    \"aquatic plants\": 4,\n",
    "    \"human divers\": 5,\n",
    "    \"robots\": 6,\n",
    "    \"sea-floor\": 7,\n",
    "}\n",
    "\n",
    "CATEGORY_TO_PROMPT = {v: k for k, v in PROMPT_TO_CATEGORY.items()}\n",
    "\n",
    "\n",
    "def polygon_to_rle(segmentation, img_height, img_width):\n",
    "    # segmentation may be: [x1,y1,...] or [[...]] or list of polygons\n",
    "    if len(segmentation) == 0:\n",
    "        return None\n",
    "\n",
    "    # Normalize to list of polygons\n",
    "    if isinstance(segmentation[0], list):\n",
    "        polys = segmentation\n",
    "    else:\n",
    "        polys = [segmentation]\n",
    "\n",
    "    mask = Image.new(\"L\", (img_width, img_height), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    for poly in polys:\n",
    "        coords = np.array(poly).reshape(-1, 2)\n",
    "        draw.polygon(coords.flatten().tolist(), outline=1, fill=1)\n",
    "\n",
    "    mask_np = np.array(mask, dtype=np.uint8)\n",
    "    rle = mask_utils.encode(np.asfortranarray(mask_np))\n",
    "    rle[\"counts\"] = rle[\"counts\"].decode(\"ascii\")\n",
    "\n",
    "    return rle\n",
    "\n",
    "\n",
    "def group_items_to_list(items):\n",
    "    grouped = defaultdict(lambda: defaultdict(lambda: {\"boxes\": [], \"masks_rle\": []}))\n",
    "\n",
    "    for d in items:\n",
    "        img_path = d[\"img_path\"]\n",
    "        img_loaded = Image.open(img_path)\n",
    "\n",
    "        cat = CATEGORY_TO_PROMPT[d[\"category_id\"]]\n",
    "\n",
    "        grouped[img_path][cat][\"boxes\"].append(d[\"bbox\"])\n",
    "        grouped[img_path][cat][\"masks_rle\"].append(\n",
    "            polygon_to_rle(d[\"segmentation\"], img_loaded.height, img_loaded.width)\n",
    "        )\n",
    "\n",
    "    out = []\n",
    "    for img_path, cats in grouped.items():\n",
    "        entry = {\"img_path\": img_path}\n",
    "        entry.update(cats)\n",
    "        out.append(entry)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_data(path='USIS10K\\\\multi_class_annotations\\\\multi_class_test_annotations.json'):\n",
    "    data = json.loads(open(path).read())\n",
    "    polished_data = []\n",
    "\n",
    "    for annotation in data['annotations']:\n",
    "        category_id = annotation['category_id']\n",
    "        image_id = annotation['image_id']\n",
    "        bbox = annotation['bbox']\n",
    "        segmentation = annotation['segmentation']\n",
    "        img_path = f\"USIS10K/test/{id_to_img[image_id]}\"\n",
    "\n",
    "        polished_data.append({\n",
    "            'img_path': img_path,\n",
    "            'category_id': category_id,\n",
    "            'bbox': bbox,\n",
    "            'segmentation': segmentation\n",
    "        })\n",
    "\n",
    "    return group_items_to_list(polished_data)\n",
    "\n",
    "def load_usis_preds_data(path='usis_sam_preds_rle.json'):\n",
    "    data = json.loads(open(path).read())\n",
    "    polished_data = []\n",
    "\n",
    "    for annotation in data['predictions']:\n",
    "        category_id = annotation['category_id']\n",
    "        image_id = annotation['image_id']\n",
    "        bbox = annotation['bbox']\n",
    "        segmentation = annotation['segmentation']\n",
    "        img_path = f\"USIS10K/test/{id_to_img[image_id]}\"\n",
    "\n",
    "        polished_data.append({\n",
    "            'img_path': img_path,\n",
    "            'category_id': category_id,\n",
    "            'bbox': bbox,\n",
    "            'segmentation': segmentation\n",
    "        })\n",
    "\n",
    "    return polished_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd142fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in dataset:\n",
      "  ID 1: wrecks/ruins\n",
      "  ID 2: fish\n",
      "  ID 3: reefs\n",
      "  ID 4: aquatic plants\n",
      "  ID 5: human divers\n",
      "  ID 6: robots\n",
      "  ID 7: sea-floor\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pycocotools import mask as mask_utils\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Category mapping: prompt name -> category_id in USIS10K\n",
    "# You may need to adjust this based on actual category IDs in your dataset\n",
    "PROMPT_TO_CATEGORY = {\n",
    "    \"wrecks/ruins\": 1,\n",
    "    \"fish\": 2,  \n",
    "    \"reefs\": 3,\n",
    "    \"aquatic plants\": 4,\n",
    "    \"human divers\": 5,\n",
    "    \"robots\": 6,\n",
    "    \"sea-floor\": 7,\n",
    "}\n",
    "\n",
    "# Check actual category IDs from the dataset\n",
    "def get_category_mapping():\n",
    "    data = json.load(open('USIS10K\\\\multi_class_annotations\\\\multi_class_test_annotations.json'))\n",
    "    print(\"Categories in dataset:\")\n",
    "    for cat in data.get('categories', []):\n",
    "        print(f\"  ID {cat['id']}: {cat['name']}\")\n",
    "    return {cat['name']: cat['id'] for cat in data.get('categories', [])}\n",
    "\n",
    "category_map = get_category_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2c11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth in COCO format for evaluation\n",
    "gt_data = load_data()\n",
    "usis_data = load_usis_preds_data()\n",
    "predictions = json.load(open('sam3_usis10k_preds_rle.json'))\n",
    "for pred in predictions:\n",
    "    pred['img_path'] = pred['img_path'].replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87745ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TO_CATEGORY = {\n",
    "    \"wrecks/ruins\": 1,\n",
    "    \"fish\": 2,  \n",
    "    \"reefs\": 3,\n",
    "    \"aquatic plants\": 4,\n",
    "    \"human divers\": 5,\n",
    "    \"robots\": 6,\n",
    "    \"sea-floor\": 7,\n",
    "}\n",
    "\n",
    "CATEGORY_TO_PROMPT = {v: k for k, v in PROMPT_TO_CATEGORY.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "500b17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two boxes.\n",
    "    Boxes are in format [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "    \n",
    "    if inter_x_max <= inter_x_min or inter_y_max <= inter_y_min:\n",
    "        return 0.0\n",
    "    \n",
    "    inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
    "    \n",
    "    # Calculate union\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "def gt_bbox_to_xyxy(bbox):\n",
    "    \"\"\"\n",
    "    Convert ground truth bbox from [x, y, width, height] to [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "def polygon_to_rle(segmentation, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Convert polygon segmentation to RLE format\n",
    "    \"\"\"\n",
    "    rles = mask_utils.frPyObjects(segmentation, img_height, img_width)\n",
    "    rle = mask_utils.merge(rles)\n",
    "    return rle\n",
    "\n",
    "\n",
    "def calculate_mask_iou(rle1, rle2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two masks in RLE format\n",
    "    \"\"\"\n",
    "    iou = mask_utils.iou([rle1], [rle2], [0])\n",
    "    return iou[0][0]\n",
    "\n",
    "def evaluate_recall(predictions, gt_data, iou_threshold=0.5, ignore_labels=False):\n",
    "    results = {\n",
    "        'overall': {'total_gt': 0, 'detected': 0, 'recall': 0.0},\n",
    "        'per_category': {}\n",
    "    }\n",
    "\n",
    "    for pred_item, gt_item in zip(predictions, gt_data):\n",
    "        # Iterate through every category in this ground truth item\n",
    "        for gt_category_name, gt_category_data in gt_item.items():\n",
    "            if gt_category_name == \"img_path\":\n",
    "                continue\n",
    "\n",
    "            gt_boxes = gt_category_data[\"boxes\"]\n",
    "\n",
    "            # Ensure category stats exist\n",
    "            if gt_category_name not in results['per_category']:\n",
    "                results['per_category'][gt_category_name] = {\n",
    "                    'total_gt': 0, 'detected': 0, 'recall': 0.0\n",
    "                }\n",
    "\n",
    "            for gt_box in gt_boxes:\n",
    "                gt_xyxy = gt_bbox_to_xyxy(gt_box)\n",
    "\n",
    "                results['overall']['total_gt'] += 1\n",
    "                results['per_category'][gt_category_name]['total_gt'] += 1\n",
    "\n",
    "                detected = False\n",
    "\n",
    "                if ignore_labels:\n",
    "                    # search in ALL predicted categories\n",
    "                    for pred_cat_name, pred_data in pred_item.items():\n",
    "                        if pred_cat_name == \"img_path\":\n",
    "                            continue\n",
    "                        for pred_box in pred_data[\"boxes\"]:\n",
    "                            if calculate_iou(pred_box, gt_xyxy) >= iou_threshold:\n",
    "                                detected = True\n",
    "                                break\n",
    "                        if detected:\n",
    "                            break\n",
    "                else:\n",
    "                    # search only in the SAME predicted category\n",
    "                    if gt_category_name in pred_item:\n",
    "                        for pred_box in pred_item[gt_category_name][\"boxes\"]:\n",
    "                            if calculate_iou(pred_box, gt_xyxy) >= iou_threshold:\n",
    "                                detected = True\n",
    "                                break\n",
    "\n",
    "                if detected:\n",
    "                    results['overall']['detected'] += 1\n",
    "                    results['per_category'][gt_category_name]['detected'] += 1\n",
    "\n",
    "    # Final recall calculations\n",
    "    if results['overall']['total_gt'] > 0:\n",
    "        results['overall']['recall'] = (\n",
    "            results['overall']['detected'] / results['overall']['total_gt']\n",
    "        )\n",
    "\n",
    "    for cat_name, s in results['per_category'].items():\n",
    "        if s['total_gt'] > 0:\n",
    "            s['recall'] = s['detected'] / s['total_gt']\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9794dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sam3 = evaluate_recall(predictions, gt_data, iou_threshold=0.5, ignore_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a77a53c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': {'total_gt': 2860, 'detected': 998, 'recall': 0.34895104895104895},\n",
       " 'per_category': {'fish': {'total_gt': 1566,\n",
       "   'detected': 593,\n",
       "   'recall': 0.3786717752234994},\n",
       "  'reefs': {'total_gt': 759, 'detected': 292, 'recall': 0.3847167325428195},\n",
       "  'human divers': {'total_gt': 193,\n",
       "   'detected': 77,\n",
       "   'recall': 0.39896373056994816},\n",
       "  'wrecks/ruins': {'total_gt': 153, 'detected': 0, 'recall': 0.0},\n",
       "  'robots': {'total_gt': 47, 'detected': 19, 'recall': 0.40425531914893614},\n",
       "  'sea-floor': {'total_gt': 68, 'detected': 11, 'recall': 0.16176470588235295},\n",
       "  'aquatic plants': {'total_gt': 74,\n",
       "   'detected': 6,\n",
       "   'recall': 0.08108108108108109}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sam3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6100f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sam3_eval_results.json\", \"w\") as f:\n",
    "    json.dump(results_sam3, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
