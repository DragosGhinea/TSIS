{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b105949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(open('USIS10K\\\\multi_class_annotations\\\\multi_class_test_annotations.json').read())\n",
    "\n",
    "id_to_img = {}\n",
    "\n",
    "for img in data['images']:\n",
    "    id_to_img[img['id']] = img['file_name']\n",
    "\n",
    "def load_data(path='USIS10K\\\\multi_class_annotations\\\\multi_class_test_annotations.json'):\n",
    "    data = json.loads(open(path).read())\n",
    "    polished_data = []\n",
    "\n",
    "    for annotation in data['annotations']:\n",
    "        category_id = annotation['category_id']\n",
    "        image_id = annotation['image_id']\n",
    "        bbox = annotation['bbox']\n",
    "        segmentation = annotation['segmentation']\n",
    "        img_path = f\"USIS10K/test/{id_to_img[image_id]}\"\n",
    "\n",
    "        polished_data.append({\n",
    "            'img_path': img_path,\n",
    "            'category_id': category_id,\n",
    "            'bbox': bbox,\n",
    "            'segmentation': segmentation\n",
    "        })\n",
    "\n",
    "    return polished_data\n",
    "\n",
    "def load_usis_preds_data(path='usis_sam_preds_rle.json'):\n",
    "    data = json.loads(open(path).read())\n",
    "    polished_data = []\n",
    "\n",
    "    for annotation in data['predictions']:\n",
    "        category_id = annotation['category_id']\n",
    "        image_id = annotation['image_id']\n",
    "        bbox = annotation['bbox']\n",
    "        segmentation = annotation['segmentation']\n",
    "        img_path = f\"USIS10K/test/{id_to_img[image_id]}\"\n",
    "\n",
    "        polished_data.append({\n",
    "            'img_path': img_path,\n",
    "            'category_id': category_id,\n",
    "            'bbox': bbox,\n",
    "            'segmentation': segmentation\n",
    "        })\n",
    "\n",
    "    return polished_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd142fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in dataset:\n",
      "  ID 1: wrecks/ruins\n",
      "  ID 2: fish\n",
      "  ID 3: reefs\n",
      "  ID 4: aquatic plants\n",
      "  ID 5: human divers\n",
      "  ID 6: robots\n",
      "  ID 7: sea-floor\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pycocotools import mask as mask_utils\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Category mapping: prompt name -> category_id in USIS10K\n",
    "# You may need to adjust this based on actual category IDs in your dataset\n",
    "PROMPT_TO_CATEGORY = {\n",
    "    \"wrecks/ruins\": 1,\n",
    "    \"fish\": 2,  \n",
    "    \"reefs\": 3,\n",
    "    \"aquatic plants\": 4,\n",
    "    \"human divers\": 5,\n",
    "    \"robots\": 6,\n",
    "    \"sea-floor\": 7,\n",
    "}\n",
    "\n",
    "# Check actual category IDs from the dataset\n",
    "def get_category_mapping():\n",
    "    data = json.load(open('USIS10K\\\\multi_class_annotations\\\\multi_class_test_annotations.json'))\n",
    "    print(\"Categories in dataset:\")\n",
    "    for cat in data.get('categories', []):\n",
    "        print(f\"  ID {cat['id']}: {cat['name']}\")\n",
    "    return {cat['name']: cat['id'] for cat in data.get('categories', [])}\n",
    "\n",
    "category_map = get_category_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2c11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth in COCO format for evaluation\n",
    "gt_data = load_data()\n",
    "usis_data = load_usis_preds_data()\n",
    "predictions = json.load(open('sam3_usis10k_preds_rle_simple_prompt.json'))\n",
    "for pred in predictions:\n",
    "    pred['img_path'] = pred['img_path'].replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87745ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TO_CATEGORY = {\n",
    "    \"wrecks/ruins\": 1,\n",
    "    \"fish\": 2,  \n",
    "    \"reefs\": 3,\n",
    "    \"aquatic plants\": 4,\n",
    "    \"human divers\": 5,\n",
    "    \"robots\": 6,\n",
    "    \"sea-floor\": 7,\n",
    "}\n",
    "\n",
    "CATEGORY_TO_PROMPT = {v: k for k, v in PROMPT_TO_CATEGORY.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e31fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_usis_to_prediction_format(usis_data):\n",
    "    \"\"\"\n",
    "    Convert USIS predictions from per-annotation format to grouped format matching SAM3 predictions.\n",
    "    \n",
    "    Input format (usis_data):\n",
    "    [\n",
    "        {'img_path': '...', 'category_id': 1, 'bbox': [x,y,w,h], 'segmentation': {...}},\n",
    "        {'img_path': '...', 'category_id': 2, 'bbox': [x,y,w,h], 'segmentation': {...}},\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    Output format (matching predictions):\n",
    "    [\n",
    "        {\n",
    "            'img_path': '...',\n",
    "            'fish': {'boxes': [[x1,y1,x2,y2], ...], 'scores': [...], 'masks_rle': [...]},\n",
    "            'reefs': {'boxes': [...], 'scores': [...], 'masks_rle': [...]},\n",
    "            ...\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Group annotations by image path\n",
    "    img_annotations = defaultdict(list)\n",
    "    for ann in usis_data:\n",
    "        img_annotations[ann['img_path']].append(ann)\n",
    "    \n",
    "    # Convert to prediction format\n",
    "    converted = []\n",
    "    for img_path, annotations in img_annotations.items():\n",
    "        pred = {'img_path': img_path}\n",
    "        \n",
    "        # Initialize all categories with empty lists\n",
    "        for prompt in PROMPT_TO_CATEGORY.keys():\n",
    "            pred[prompt] = {'boxes': [], 'scores': [], 'masks_rle': []}\n",
    "        \n",
    "        # Fill in annotations\n",
    "        for ann in annotations:\n",
    "            category_id = ann['category_id'] + 1\n",
    "            prompt_name = CATEGORY_TO_PROMPT.get(category_id, f\"unknown_{category_id}\")\n",
    "            \n",
    "            if prompt_name not in pred:\n",
    "                pred[prompt_name] = {'boxes': [], 'scores': [], 'masks_rle': []}\n",
    "            \n",
    "            # Convert bbox from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            box_xyxy = [x, y, x + w, y + h]\n",
    "            pred[prompt_name]['boxes'].append(box_xyxy)\n",
    "            \n",
    "            # USIS predictions don't have scores, use 1.0 as default\n",
    "            pred[prompt_name]['scores'].append(1.0)\n",
    "            \n",
    "            # Add segmentation mask (already in RLE format)\n",
    "            if 'segmentation' in ann and ann['segmentation']:\n",
    "                pred[prompt_name]['masks_rle'].append(ann['segmentation'])\n",
    "        \n",
    "        converted.append(pred)\n",
    "    \n",
    "    return converted\n",
    "\n",
    "\n",
    "def align_predictions_by_image(predictions, usis_predictions):\n",
    "    \"\"\"\n",
    "    Align SAM3 predictions and USIS predictions by image path so they can be compared.\n",
    "    Returns two lists with matching order based on SAM3 predictions order.\n",
    "    \"\"\"\n",
    "    # Create lookup for USIS predictions by image path\n",
    "    usis_by_path = {p['img_path']: p for p in usis_predictions}\n",
    "    \n",
    "    aligned_usis = []\n",
    "    for pred in predictions:\n",
    "        img_path = pred['img_path']\n",
    "        if img_path in usis_by_path:\n",
    "            aligned_usis.append(usis_by_path[img_path])\n",
    "        else:\n",
    "            # Create empty prediction if not found\n",
    "            empty_pred = {'img_path': img_path}\n",
    "            for prompt in PROMPT_TO_CATEGORY.keys():\n",
    "                empty_pred[prompt] = {'boxes': [], 'scores': [], 'masks_rle': []}\n",
    "            aligned_usis.append(empty_pred)\n",
    "    \n",
    "    return aligned_usis\n",
    "\n",
    "\n",
    "# Convert USIS data to match prediction format\n",
    "usis_predictions = convert_usis_to_prediction_format(usis_data)\n",
    "\n",
    "# Align USIS predictions with SAM3 predictions order\n",
    "usis_predictions_aligned = align_predictions_by_image(predictions, usis_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ded20692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two boxes.\n",
    "    Boxes are in format [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "    \n",
    "    if inter_x_max <= inter_x_min or inter_y_max <= inter_y_min:\n",
    "        return 0.0\n",
    "    \n",
    "    inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
    "    \n",
    "    # Calculate union\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "\n",
    "def gt_bbox_to_xyxy(bbox):\n",
    "    \"\"\"\n",
    "    Convert ground truth bbox from [x, y, width, height] to [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "def polygon_to_rle(segmentation, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Convert polygon segmentation to RLE format\n",
    "    \"\"\"\n",
    "    rles = mask_utils.frPyObjects(segmentation, img_height, img_width)\n",
    "    rle = mask_utils.merge(rles)\n",
    "    return rle\n",
    "\n",
    "\n",
    "def calculate_mask_iou(rle1, rle2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two masks in RLE format\n",
    "    \"\"\"\n",
    "    iou = mask_utils.iou([rle1], [rle2], [0])\n",
    "    return iou[0][0]\n",
    "\n",
    "\n",
    "def evaluate_recall(predictions, gt_data, iou_threshold=0.5, ignore_labels=False):\n",
    "    \"\"\"\n",
    "    Evaluate recall: what fraction of ground truth boxes are detected?\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction dictionaries\n",
    "        gt_data: List of ground truth dictionaries\n",
    "        iou_threshold: IoU threshold to consider a detection as correct\n",
    "        ignore_labels: If True, check if any predicted box matches GT box regardless of category\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with overall and per-category recall metrics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'overall': {'total_gt': 0, 'detected': 0, 'recall': 0.0},\n",
    "        'per_category': {}\n",
    "    }\n",
    "    \n",
    "    for pred, gt in zip(predictions, gt_data):\n",
    "        # Get ground truth category and bbox\n",
    "        gt_category_id = gt['category_id']\n",
    "        gt_category_name = CATEGORY_TO_PROMPT.get(gt_category_id, f\"unknown_{gt_category_id}\")\n",
    "        gt_bbox_xyxy = gt_bbox_to_xyxy(gt['bbox'])\n",
    "        \n",
    "        # Initialize category stats if needed\n",
    "        if gt_category_name not in results['per_category']:\n",
    "            results['per_category'][gt_category_name] = {\n",
    "                'total_gt': 0, 'detected': 0, 'recall': 0.0\n",
    "            }\n",
    "        \n",
    "        # Update counts\n",
    "        results['overall']['total_gt'] += 1\n",
    "        results['per_category'][gt_category_name]['total_gt'] += 1\n",
    "        \n",
    "        # Check if any predicted box matches the ground truth\n",
    "        detected = False\n",
    "        \n",
    "        if ignore_labels:\n",
    "            # Check all predicted boxes across all categories\n",
    "            for category_name, category_data in pred.items():\n",
    "                if category_name == 'img_path':\n",
    "                    continue\n",
    "                pred_boxes = category_data.get('boxes', [])\n",
    "                for pred_box in pred_boxes:\n",
    "                    iou = calculate_iou(pred_box, gt_bbox_xyxy)\n",
    "                    if iou >= iou_threshold:\n",
    "                        detected = True\n",
    "                        break\n",
    "                if detected:\n",
    "                    break\n",
    "        else:\n",
    "            # Only check predicted boxes for the matching category\n",
    "            if gt_category_name in pred:\n",
    "                pred_boxes = pred[gt_category_name]['boxes']\n",
    "                for pred_box in pred_boxes:\n",
    "                    iou = calculate_iou(pred_box, gt_bbox_xyxy)\n",
    "                    if iou >= iou_threshold:\n",
    "                        detected = True\n",
    "                        break\n",
    "        \n",
    "        if detected:\n",
    "            results['overall']['detected'] += 1\n",
    "            results['per_category'][gt_category_name]['detected'] += 1\n",
    "    \n",
    "    # Calculate recall percentages\n",
    "    if results['overall']['total_gt'] > 0:\n",
    "        results['overall']['recall'] = results['overall']['detected'] / results['overall']['total_gt']\n",
    "    \n",
    "    for category in results['per_category']:\n",
    "        cat_stats = results['per_category'][category]\n",
    "        if cat_stats['total_gt'] > 0:\n",
    "            cat_stats['recall'] = cat_stats['detected'] / cat_stats['total_gt']\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_recall_with_segmentation(predictions, gt_data, bbox_iou_threshold=0.5, \n",
    "                                      mask_iou_threshold=0.5, ignore_labels=False,\n",
    "                                      img_height=480, img_width=640):\n",
    "    \"\"\"\n",
    "    Evaluate recall for both bounding boxes and segmentation masks.\n",
    "    For each GT box that's detected, also check if the segmentation mask matches.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction dictionaries\n",
    "        gt_data: List of ground truth dictionaries\n",
    "        bbox_iou_threshold: IoU threshold for bounding box matching\n",
    "        mask_iou_threshold: IoU threshold for segmentation mask matching\n",
    "        ignore_labels: If True, check any predicted box/mask regardless of category\n",
    "        img_height: Image height for mask conversion\n",
    "        img_width: Image width for mask conversion\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with bbox and segmentation recall metrics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'bbox': {'total_gt': 0, 'detected': 0, 'recall': 0.0},\n",
    "        'segmentation': {'total_gt': 0, 'detected': 0, 'recall': 0.0},\n",
    "        'per_category': {}\n",
    "    }\n",
    "    \n",
    "    for pred, gt in zip(predictions, gt_data):\n",
    "        # Get ground truth category and bbox\n",
    "        gt_category_id = gt['category_id']\n",
    "        gt_category_name = CATEGORY_TO_PROMPT.get(gt_category_id, f\"unknown_{gt_category_id}\")\n",
    "        gt_bbox_xyxy = gt_bbox_to_xyxy(gt['bbox'])\n",
    "        \n",
    "        # Convert GT segmentation to RLE if available\n",
    "        gt_has_segmentation = 'segmentation' in gt and gt['segmentation']\n",
    "        if gt_has_segmentation:\n",
    "            gt_rle = polygon_to_rle(gt['segmentation'], img_height, img_width)\n",
    "        \n",
    "        # Initialize category stats if needed\n",
    "        if gt_category_name not in results['per_category']:\n",
    "            results['per_category'][gt_category_name] = {\n",
    "                'bbox': {'total_gt': 0, 'detected': 0, 'recall': 0.0},\n",
    "                'segmentation': {'total_gt': 0, 'detected': 0, 'recall': 0.0}\n",
    "            }\n",
    "        \n",
    "        # Update counts\n",
    "        results['bbox']['total_gt'] += 1\n",
    "        results['per_category'][gt_category_name]['bbox']['total_gt'] += 1\n",
    "        \n",
    "        if gt_has_segmentation:\n",
    "            results['segmentation']['total_gt'] += 1\n",
    "            results['per_category'][gt_category_name]['segmentation']['total_gt'] += 1\n",
    "        \n",
    "        # Check if any predicted box matches the ground truth\n",
    "        bbox_detected = False\n",
    "        mask_detected = False\n",
    "        matched_pred_idx = -1\n",
    "        matched_category = None\n",
    "        \n",
    "        if ignore_labels:\n",
    "            # Check all predicted boxes across all categories\n",
    "            for category_name, category_data in pred.items():\n",
    "                if category_name == 'img_path':\n",
    "                    continue\n",
    "                pred_boxes = category_data.get('boxes', [])\n",
    "                for idx, pred_box in enumerate(pred_boxes):\n",
    "                    iou = calculate_iou(pred_box, gt_bbox_xyxy)\n",
    "                    if iou >= bbox_iou_threshold:\n",
    "                        bbox_detected = True\n",
    "                        matched_pred_idx = idx\n",
    "                        matched_category = category_name\n",
    "                        break\n",
    "                if bbox_detected:\n",
    "                    break\n",
    "        else:\n",
    "            # Only check predicted boxes for the matching category\n",
    "            if gt_category_name in pred:\n",
    "                pred_boxes = pred[gt_category_name]['boxes']\n",
    "                for idx, pred_box in enumerate(pred_boxes):\n",
    "                    iou = calculate_iou(pred_box, gt_bbox_xyxy)\n",
    "                    if iou >= bbox_iou_threshold:\n",
    "                        bbox_detected = True\n",
    "                        matched_pred_idx = idx\n",
    "                        matched_category = gt_category_name\n",
    "                        break\n",
    "        \n",
    "        # Update bbox detection counts\n",
    "        if bbox_detected:\n",
    "            results['bbox']['detected'] += 1\n",
    "            results['per_category'][gt_category_name]['bbox']['detected'] += 1\n",
    "            \n",
    "            # If bbox was detected and GT has segmentation, check mask IoU\n",
    "            if gt_has_segmentation and matched_category and matched_pred_idx >= 0:\n",
    "                pred_masks_rle = pred[matched_category].get('masks_rle', [])\n",
    "                if matched_pred_idx < len(pred_masks_rle):\n",
    "                    pred_rle = pred_masks_rle[matched_pred_idx]\n",
    "                    mask_iou = calculate_mask_iou(pred_rle, gt_rle)\n",
    "                    \n",
    "                    if mask_iou >= mask_iou_threshold:\n",
    "                        mask_detected = True\n",
    "                        results['segmentation']['detected'] += 1\n",
    "                        results['per_category'][gt_category_name]['segmentation']['detected'] += 1\n",
    "    \n",
    "    # Calculate recall percentages\n",
    "    if results['bbox']['total_gt'] > 0:\n",
    "        results['bbox']['recall'] = results['bbox']['detected'] / results['bbox']['total_gt']\n",
    "    \n",
    "    if results['segmentation']['total_gt'] > 0:\n",
    "        results['segmentation']['recall'] = results['segmentation']['detected'] / results['segmentation']['total_gt']\n",
    "    \n",
    "    for category in results['per_category']:\n",
    "        cat_stats = results['per_category'][category]\n",
    "        if cat_stats['bbox']['total_gt'] > 0:\n",
    "            cat_stats['bbox']['recall'] = cat_stats['bbox']['detected'] / cat_stats['bbox']['total_gt']\n",
    "        if cat_stats['segmentation']['total_gt'] > 0:\n",
    "            cat_stats['segmentation']['recall'] = cat_stats['segmentation']['detected'] / cat_stats['segmentation']['total_gt']\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results):\n",
    "    \"\"\"Pretty print evaluation results\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RECALL EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nOverall Recall:\")\n",
    "    print(f\"  Detected: {results['overall']['detected']}/{results['overall']['total_gt']}\")\n",
    "    print(f\"  Recall: {results['overall']['recall']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nPer-Category Recall:\")\n",
    "    for category, stats in sorted(results['per_category'].items()):\n",
    "        print(f\"  {category}:\")\n",
    "        print(f\"    Detected: {stats['detected']}/{stats['total_gt']}\")\n",
    "        print(f\"    Recall: {stats['recall']:.2%}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def print_segmentation_results(results):\n",
    "    \"\"\"Pretty print evaluation results including segmentation\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BBOX AND SEGMENTATION RECALL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nOverall Bounding Box Recall:\")\n",
    "    print(f\"  Detected: {results['bbox']['detected']}/{results['bbox']['total_gt']}\")\n",
    "    print(f\"  Recall: {results['bbox']['recall']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nOverall Segmentation Recall:\")\n",
    "    print(f\"  Detected: {results['segmentation']['detected']}/{results['segmentation']['total_gt']}\")\n",
    "    print(f\"  Recall: {results['segmentation']['recall']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nPer-Category Results:\")\n",
    "    for category, stats in sorted(results['per_category'].items()):\n",
    "        print(f\"\\n  {category}:\")\n",
    "        print(f\"    BBox - Detected: {stats['bbox']['detected']}/{stats['bbox']['total_gt']}, \"\n",
    "              f\"Recall: {stats['bbox']['recall']:.2%}\")\n",
    "        print(f\"    Seg  - Detected: {stats['segmentation']['detected']}/{stats['segmentation']['total_gt']}, \"\n",
    "              f\"Recall: {stats['segmentation']['recall']:.2%}\")\n",
    "    print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54c20fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(results1, results2, name1=\"Results 1\", name2=\"Results 2\"):\n",
    "    \"\"\"\n",
    "    Compare two detection results dictionaries and display differences.\n",
    "    \n",
    "    Args:\n",
    "        results1: First results dictionary\n",
    "        results2: Second results dictionary\n",
    "        name1: Label for first results\n",
    "        name2: Label for second results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPARISON: {name1} vs {name2}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Compare overall metrics\n",
    "    print(\"OVERALL METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    overall1 = results1['overall']\n",
    "    overall2 = results2['overall']\n",
    "    \n",
    "    for metric in ['total_gt', 'detected', 'recall']:\n",
    "        val1 = overall1[metric]\n",
    "        val2 = overall2[metric]\n",
    "        diff = val2 - val1\n",
    "        \n",
    "        if metric == 'recall':\n",
    "            pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "            print(f\"{metric:12s}: {val1:.4f} → {val2:.4f} (Δ {diff:+.4f}, {pct_diff:+.2f}%)\")\n",
    "        else:\n",
    "            pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "            print(f\"{metric:12s}: {val1:6d} → {val2:6d} (Δ {diff:+6d}, {pct_diff:+.2f}%)\")\n",
    "    \n",
    "    # Compare per-category metrics\n",
    "    print(\"\\n\\nPER-CATEGORY METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get all categories from both results\n",
    "    categories1 = set(results1['per_category'].keys())\n",
    "    categories2 = set(results2['per_category'].keys())\n",
    "    all_categories = sorted(categories1 | categories2)\n",
    "    \n",
    "    for category in all_categories:\n",
    "        cat1 = results1['per_category'].get(category)\n",
    "        cat2 = results2['per_category'].get(category)\n",
    "        \n",
    "        print(f\"\\n{category.upper()}\")\n",
    "        \n",
    "        if cat1 is None:\n",
    "            print(f\"  Only in {name2}: {cat2}\")\n",
    "            continue\n",
    "        if cat2 is None:\n",
    "            print(f\"  Only in {name1}: {cat1}\")\n",
    "            continue\n",
    "        \n",
    "        for metric in ['total_gt', 'detected', 'recall']:\n",
    "            val1 = cat1[metric]\n",
    "            val2 = cat2[metric]\n",
    "            diff = val2 - val1\n",
    "            \n",
    "            if metric == 'recall':\n",
    "                pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "                print(f\"  {metric:12s}: {val1:.4f} → {val2:.4f} (Δ {diff:+.4f}, {pct_diff:+.2f}%)\")\n",
    "            else:\n",
    "                pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "                print(f\"  {metric:12s}: {val1:6d} → {val2:6d} (Δ {diff:+6d}, {pct_diff:+.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49d655cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: SAM3 Predictions vs USIS Predictions\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "total_gt    :   2860 →   2860 (Δ     +0, +0.00%)\n",
      "detected    :   1548 →   2364 (Δ   +816, +52.71%)\n",
      "recall      : 0.5413 → 0.8266 (Δ +0.2853, +52.71%)\n",
      "\n",
      "\n",
      "PER-CATEGORY METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "AQUATIC PLANTS\n",
      "  total_gt    :     74 →     74 (Δ     +0, +0.00%)\n",
      "  detected    :     32 →     22 (Δ    -10, -31.25%)\n",
      "  recall      : 0.4324 → 0.2973 (Δ -0.1351, -31.25%)\n",
      "\n",
      "FISH\n",
      "  total_gt    :   1566 →   1566 (Δ     +0, +0.00%)\n",
      "  detected    :   1365 →   1414 (Δ    +49, +3.59%)\n",
      "  recall      : 0.8716 → 0.9029 (Δ +0.0313, +3.59%)\n",
      "\n",
      "HUMAN DIVERS\n",
      "  total_gt    :    193 →    193 (Δ     +0, +0.00%)\n",
      "  detected    :      2 →    181 (Δ   +179, +8950.00%)\n",
      "  recall      : 0.0104 → 0.9378 (Δ +0.9275, +8950.00%)\n",
      "\n",
      "REEFS\n",
      "  total_gt    :    759 →    759 (Δ     +0, +0.00%)\n",
      "  detected    :    141 →    564 (Δ   +423, +300.00%)\n",
      "  recall      : 0.1858 → 0.7431 (Δ +0.5573, +300.00%)\n",
      "\n",
      "ROBOTS\n",
      "  total_gt    :     47 →     47 (Δ     +0, +0.00%)\n",
      "  detected    :      4 →     34 (Δ    +30, +750.00%)\n",
      "  recall      : 0.0851 → 0.7234 (Δ +0.6383, +750.00%)\n",
      "\n",
      "SEA-FLOOR\n",
      "  total_gt    :     68 →     68 (Δ     +0, +0.00%)\n",
      "  detected    :      0 →     28 (Δ    +28, +0.00%)\n",
      "  recall      : 0.0000 → 0.4118 (Δ +0.4118, +0.00%)\n",
      "\n",
      "WRECKS/RUINS\n",
      "  total_gt    :    153 →    153 (Δ     +0, +0.00%)\n",
      "  detected    :      4 →    121 (Δ   +117, +2925.00%)\n",
      "  recall      : 0.0261 → 0.7908 (Δ +0.7647, +2925.00%)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_sam3 = evaluate_recall(predictions, gt_data, iou_threshold=0.5)\n",
    "results_usis = evaluate_recall(usis_predictions_aligned, gt_data, iou_threshold=0.5)\n",
    "\n",
    "compare_results(results_sam3, results_usis, name1=\"SAM3 Predictions\", name2=\"USIS Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9794dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: SAM3 Predictions vs USIS Predictions\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "total_gt    :   2860 →   2860 (Δ     +0, +0.00%)\n",
      "detected    :   1728 →   2515 (Δ   +787, +45.54%)\n",
      "recall      : 0.6042 → 0.8794 (Δ +0.2752, +45.54%)\n",
      "\n",
      "\n",
      "PER-CATEGORY METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "AQUATIC PLANTS\n",
      "  total_gt    :     74 →     74 (Δ     +0, +0.00%)\n",
      "  detected    :     36 →     53 (Δ    +17, +47.22%)\n",
      "  recall      : 0.4865 → 0.7162 (Δ +0.2297, +47.22%)\n",
      "\n",
      "FISH\n",
      "  total_gt    :   1566 →   1566 (Δ     +0, +0.00%)\n",
      "  detected    :   1379 →   1449 (Δ    +70, +5.08%)\n",
      "  recall      : 0.8806 → 0.9253 (Δ +0.0447, +5.08%)\n",
      "\n",
      "HUMAN DIVERS\n",
      "  total_gt    :    193 →    193 (Δ     +0, +0.00%)\n",
      "  detected    :      2 →    188 (Δ   +186, +9300.00%)\n",
      "  recall      : 0.0104 → 0.9741 (Δ +0.9637, +9300.00%)\n",
      "\n",
      "REEFS\n",
      "  total_gt    :    759 →    759 (Δ     +0, +0.00%)\n",
      "  detected    :    273 →    602 (Δ   +329, +120.51%)\n",
      "  recall      : 0.3597 → 0.7931 (Δ +0.4335, +120.51%)\n",
      "\n",
      "ROBOTS\n",
      "  total_gt    :     47 →     47 (Δ     +0, +0.00%)\n",
      "  detected    :      6 →     41 (Δ    +35, +583.33%)\n",
      "  recall      : 0.1277 → 0.8723 (Δ +0.7447, +583.33%)\n",
      "\n",
      "SEA-FLOOR\n",
      "  total_gt    :     68 →     68 (Δ     +0, +0.00%)\n",
      "  detected    :     14 →     42 (Δ    +28, +200.00%)\n",
      "  recall      : 0.2059 → 0.6176 (Δ +0.4118, +200.00%)\n",
      "\n",
      "WRECKS/RUINS\n",
      "  total_gt    :    153 →    153 (Δ     +0, +0.00%)\n",
      "  detected    :     18 →    140 (Δ   +122, +677.78%)\n",
      "  recall      : 0.1176 → 0.9150 (Δ +0.7974, +677.78%)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_sam3 = evaluate_recall(predictions, gt_data, iou_threshold=0.5, ignore_labels=True)\n",
    "results_usis = evaluate_recall(usis_predictions_aligned, gt_data, iou_threshold=0.5, ignore_labels=True)\n",
    "\n",
    "compare_results(results_sam3, results_usis, name1=\"SAM3 Predictions\", name2=\"USIS Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "885d2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bbox_seg_results(results1, results2, name1=\"Results 1\", name2=\"Results 2\"):\n",
    "    \"\"\"\n",
    "    Compare two detection/segmentation results dictionaries and display differences.\n",
    "    \n",
    "    Args:\n",
    "        results1: First results dictionary with bbox and segmentation metrics\n",
    "        results2: Second results dictionary with bbox and segmentation metrics\n",
    "        name1: Label for first results\n",
    "        name2: Label for second results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"COMPARISON: {name1} vs {name2}\")\n",
    "    print(f\"{'='*90}\\n\")\n",
    "    \n",
    "    # Compare bbox and segmentation overall metrics\n",
    "    for task_type in ['bbox', 'segmentation']:\n",
    "        print(f\"{task_type.upper()} - OVERALL METRICS\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        task1 = results1[task_type]\n",
    "        task2 = results2[task_type]\n",
    "        \n",
    "        for metric in ['total_gt', 'detected', 'recall']:\n",
    "            val1 = task1[metric]\n",
    "            val2 = task2[metric]\n",
    "            diff = val2 - val1\n",
    "            \n",
    "            if metric == 'recall':\n",
    "                pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "                print(f\"  {metric:12s}: {val1:.4f} → {val2:.4f} (Δ {diff:+.4f}, {pct_diff:+.2f}%)\")\n",
    "            else:\n",
    "                pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "                print(f\"  {metric:12s}: {val1:6d} → {val2:6d} (Δ {diff:+6d}, {pct_diff:+.2f}%)\")\n",
    "        print()\n",
    "    \n",
    "    # Compare per-category metrics\n",
    "    print(\"\\nPER-CATEGORY METRICS\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # Get all categories from both results\n",
    "    categories1 = set(results1['per_category'].keys())\n",
    "    categories2 = set(results2['per_category'].keys())\n",
    "    all_categories = sorted(categories1 | categories2)\n",
    "    \n",
    "    for category in all_categories:\n",
    "        cat1 = results1['per_category'].get(category)\n",
    "        cat2 = results2['per_category'].get(category)\n",
    "        \n",
    "        print(f\"\\n{category.upper()}\")\n",
    "        \n",
    "        if cat1 is None:\n",
    "            print(f\"  Only in {name2}\")\n",
    "            continue\n",
    "        if cat2 is None:\n",
    "            print(f\"  Only in {name1}\")\n",
    "            continue\n",
    "        \n",
    "        for task_type in ['bbox', 'segmentation']:\n",
    "            print(f\"  {task_type}:\")\n",
    "            \n",
    "            task1 = cat1[task_type]\n",
    "            task2 = cat2[task_type]\n",
    "            \n",
    "            for metric in ['total_gt', 'detected', 'recall']:\n",
    "                val1 = task1[metric]\n",
    "                val2 = task2[metric]\n",
    "                diff = val2 - val1\n",
    "                \n",
    "                if metric == 'recall':\n",
    "                    pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "                    print(f\"    {metric:12s}: {val1:.4f} → {val2:.4f} (Δ {diff:+.4f}, {pct_diff:+.2f}%)\")\n",
    "                else:\n",
    "                    pct_diff = ((val2 - val1) / val1 * 100) if val1 != 0 else 0\n",
    "                    print(f\"    {metric:12s}: {val1:6d} → {val2:6d} (Δ {diff:+6d}, {pct_diff:+.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*90}\\n\")\n",
    "\n",
    "\n",
    "def summarize_bbox_seg_differences(results1, results2, name1=\"Results 1\", name2=\"Results 2\"):\n",
    "    \"\"\"\n",
    "    Print a concise summary highlighting the biggest differences.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"SUMMARY OF KEY DIFFERENCES: {name1} vs {name2}\")\n",
    "    print(f\"{'='*90}\\n\")\n",
    "    \n",
    "    # Overall recall improvements\n",
    "    print(\"OVERALL RECALL CHANGES:\")\n",
    "    for task_type in ['bbox', 'segmentation']:\n",
    "        r1 = results1[task_type]['recall']\n",
    "        r2 = results2[task_type]['recall']\n",
    "        diff = r2 - r1\n",
    "        pct_diff = (diff / r1 * 100) if r1 != 0 else 0\n",
    "        status = \"↑ IMPROVED\" if diff > 0 else \"↓ DECLINED\" if diff < 0 else \"→ UNCHANGED\"\n",
    "        print(f\"  {task_type:15s}: {r1:.4f} → {r2:.4f} ({diff:+.4f}, {pct_diff:+.2f}%) {status}\")\n",
    "    \n",
    "    # Find categories with biggest recall changes\n",
    "    print(\"\\n\\nCATEGORIES WITH LARGEST BBOX RECALL CHANGES:\")\n",
    "    bbox_changes = []\n",
    "    for category in results1['per_category'].keys():\n",
    "        if category in results2['per_category']:\n",
    "            r1 = results1['per_category'][category]['bbox']['recall']\n",
    "            r2 = results2['per_category'][category]['bbox']['recall']\n",
    "            diff = r2 - r1\n",
    "            bbox_changes.append((category, r1, r2, diff))\n",
    "    \n",
    "    bbox_changes.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    for cat, r1, r2, diff in bbox_changes[:5]:\n",
    "        pct_diff = (diff / r1 * 100) if r1 != 0 else 0\n",
    "        status = \"↑\" if diff > 0 else \"↓\" if diff < 0 else \"→\"\n",
    "        print(f\"  {status} {cat:20s}: {r1:.4f} → {r2:.4f} ({diff:+.4f}, {pct_diff:+.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\\nCATEGORIES WITH LARGEST SEGMENTATION RECALL CHANGES:\")\n",
    "    seg_changes = []\n",
    "    for category in results1['per_category'].keys():\n",
    "        if category in results2['per_category']:\n",
    "            r1 = results1['per_category'][category]['segmentation']['recall']\n",
    "            r2 = results2['per_category'][category]['segmentation']['recall']\n",
    "            diff = r2 - r1\n",
    "            seg_changes.append((category, r1, r2, diff))\n",
    "    \n",
    "    seg_changes.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    for cat, r1, r2, diff in seg_changes[:5]:\n",
    "        pct_diff = (diff / r1 * 100) if r1 != 0 else 0\n",
    "        status = \"↑\" if diff > 0 else \"↓\" if diff < 0 else \"→\"\n",
    "        print(f\"  {status} {cat:20s}: {r1:.4f} → {r2:.4f} ({diff:+.4f}, {pct_diff:+.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*90}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e1c7775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "SUMMARY OF KEY DIFFERENCES: SAM3 Predictions vs USIS Predictions\n",
      "==========================================================================================\n",
      "\n",
      "OVERALL RECALL CHANGES:\n",
      "  bbox           : 0.5413 → 0.8266 (+0.2853, +52.71%) ↑ IMPROVED\n",
      "  segmentation   : 0.4857 → 0.7168 (+0.2311, +47.59%) ↑ IMPROVED\n",
      "\n",
      "\n",
      "CATEGORIES WITH LARGEST BBOX RECALL CHANGES:\n",
      "  ↑ human divers        : 0.0104 → 0.9378 (+0.9275, +8950.00%)\n",
      "  ↑ wrecks/ruins        : 0.0261 → 0.7908 (+0.7647, +2925.00%)\n",
      "  ↑ robots              : 0.0851 → 0.7234 (+0.6383, +750.00%)\n",
      "  ↑ reefs               : 0.1858 → 0.7431 (+0.5573, +300.00%)\n",
      "  ↑ sea-floor           : 0.0000 → 0.4118 (+0.4118, +0.00%)\n",
      "\n",
      "\n",
      "CATEGORIES WITH LARGEST SEGMENTATION RECALL CHANGES:\n",
      "  ↑ human divers        : 0.0104 → 0.8446 (+0.8342, +8050.00%)\n",
      "  ↑ wrecks/ruins        : 0.0261 → 0.7320 (+0.7059, +2700.00%)\n",
      "  ↑ robots              : 0.0851 → 0.6170 (+0.5319, +625.00%)\n",
      "  ↑ reefs               : 0.1449 → 0.5665 (+0.4216, +290.91%)\n",
      "  ↑ sea-floor           : 0.0000 → 0.3088 (+0.3088, +0.00%)\n",
      "\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seg_results_sam3 = evaluate_recall_with_segmentation(\n",
    "    predictions, gt_data, \n",
    "    bbox_iou_threshold=0.5, \n",
    "    mask_iou_threshold=0.5,\n",
    "    ignore_labels=False\n",
    ")\n",
    "\n",
    "seg_results_usis = evaluate_recall_with_segmentation(\n",
    "    usis_predictions_aligned, gt_data, \n",
    "    bbox_iou_threshold=0.5, \n",
    "    mask_iou_threshold=0.5,\n",
    "    ignore_labels=False\n",
    ")\n",
    "\n",
    "# Full detailed comparison\n",
    "# compare_bbox_seg_results(seg_results_sam3, seg_results_usis, \"SAM3 Predictions\", \"USIS Predictions\")\n",
    "\n",
    "# Concise summary\n",
    "summarize_bbox_seg_differences(seg_results_sam3, seg_results_usis, \"SAM3 Predictions\", \"USIS Predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
